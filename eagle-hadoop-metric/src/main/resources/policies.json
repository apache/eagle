[
  {
    "name": "haStatePolicy",
    "description": "ha state check for hadoop name node",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from every a = hadoopJmxMetricEventStream[metric==\"hadoop.namenode.fsnamesystem.hastate\"] -> b = hadoopJmxMetricEventStream[metric==a.metric and b.host == a.host and (convert(a.value, \"long\\\") != convert(value, \"long\"))] within 10 min select a.host, a.value as oldHaState, b.value as newHaState, b.timestamp as timestamp, b.metric as metric, b.component as component, b.site as site insert into tmp; "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  },

  {
    "name": "capacityUsedPolicy",
    "description": "capacity usage check for hadoop cluster",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from hadoopJmxMetricEventStream[metric == \"hadoop.namenode.fsnamesystemstate.capacityused\" and convert(value, \"long\") > 0]#window.externalTime(timestamp ,10 min) select metric, host, value, timestamp, component, site insert into tmp; "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  },

  {
    "name": "lastCheckPointTimePolicy",
    "description": "last check point interval check for hadoop name node pair",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from hadoopJmxMetricEventStream[metric == \"hadoop.namenode.dfs.lastcheckpointtime\" and (convert(value, \"long\") + 18000000) < timestamp]#window.externalTime(timestamp ,10 min) select metric, host, value, timestamp, component, site insert into tmp;  "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  },

  {
    "name": "missingBlockPolicy",
    "description": "missing block number check for hadoop cluster",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from hadoopJmxMetricEventStream[metric == \"hadoop.namenode.dfs.missingblocks\" and convert(value, \"long\") > 0]#window.externalTime(timestamp ,10 min) select metric, host, value, timestamp, component, site insert into tmp;  "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  },

  {
    "name": "namenodeTxLagPolicy",
    "description": "name node tx log lag check",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from every a = hadoopJmxMetricEventStream[metric==\"hadoop.namenode.journaltransaction.lastappliedorwrittentxid\"] -> b = hadoopJmxMetricEventStream[metric==a.metric and b.host != a.host and (max(convert(a.value, \"long\")) + 100) <= max(convert(value, \"long\"))] within 5 min select a.host as hostA, a.value as transactIdA, b.host as hostB, b.value as transactIdB insert into tmp; "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  },

  {
    "name": "nodecountPolicy",
    "description": "data node number check for hadoop cluster",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from every (e1 = hadoopJmxMetricEventStream[metric == \"hadoop.namenode.fsnamesystemstate.numlivedatanodes\" ]) -> e2 = hadoopJmxMetricEventStream[metric == e1.metric and host == e1.host and (convert(e1.value, \"long\") - 5) >= convert(value, \"long\") ] within 5 min select e1.metric, e1.host, e1.value as highNum, e1.timestamp as start, e2.value as lowNum, e2.timestamp as end insert into tmp;  "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  },

  {
    "name": "nameNodeSafeModeCheckPolicy",
    "description": "safe mode check for name node",
    "definition": {
      "inputStreams": [
        "hadoopJmxMetricEventStream"
      ],
      "outputStreams": [
        "tmp"
      ],
      "type": "siddhi",
      "value": " from hadoopJmxMetricEventStream[component==\"namenode\" and metric == \"hadoop.namenode.fsnamesystemstate.fsstate\" and convert(value, \"long\") > 0]#window.externalTime(timestamp ,10 min) select metric, host, value, timestamp, component, site insert into tmp;  "
    },
    "partitionSpec": [
      {
        "streamId": "hadoopJmxMetricEventStream",
        "type": "GROUPBY",
        "columns": [
          "host"
        ]
      }
    ]
  }
]