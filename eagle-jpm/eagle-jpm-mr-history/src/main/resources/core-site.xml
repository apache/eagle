<?xml version="1.0"?>
<!-- ~ Licensed to the Apache Software Foundation (ASF) under one or more
	~ contributor license agreements. See the NOTICE file distributed with ~
	this work for additional information regarding copyright ownership. ~ The
	ASF licenses this file to You under the Apache License, Version 2.0 ~ (the
	"License"); you may not use this file except in compliance with ~ the License.
	You may obtain a copy of the License at ~ ~ http://www.apache.org/licenses/LICENSE-2.0
	~ ~ Unless required by applicable law or agreed to in writing, software ~
	distributed under the License is distributed on an "AS IS" BASIS, ~ WITHOUT
	WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. ~ See the
	License for the specific language governing permissions and ~ limitations
	under the License. -->

<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration xmlns:xi="http://www.w3.org/2001/XInclude">

<!-- i/o properties -->

  <property>
    <name>io.file.buffer.size</name>
    <value>131072</value>
    <description>The size of buffer for use in sequence files.
  The size of this buffer should probably be a multiple of hardware
  page size (4096 on Intel x86), and it determines how much data is
  buffered during read and write operations.</description>
  </property>

<property>
  <description>If users connect through a SOCKS proxy, we don't
   want their SocketFactory settings interfering with the socket
   factory associated with the actual daemons.</description>
   <name>hadoop.rpc.socket.factory.class.default</name>
   <value>org.apache.hadoop.net.StandardSocketFactory</value>
</property>

<property>
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <description>A base for other temporary directories.</description>
</property>

<property>
  <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
  <value></value>
</property>

<property>
  <name>hadoop.rpc.socket.factory.class.JobSubmissionProtocol</name>
  <value></value>
</property>
              
  <property>
    <name>io.serializations</name>
    <value>org.apache.hadoop.io.serializer.WritableSerialization</value>
  </property>

  <property>
    <name>io.compression.codecs</name>
    <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
  </property>

  <!-- LZO: see http://www.facebook.com/notes/cloudera/hadoop-at-twitter-part-1-splittable-lzo-compression/178581952002 -->
  <property>
      <name>io.compression.codec.lzo.class</name>
      <value>com.hadoop.compression.lzo.LzoCodec</value>
  </property>


<!-- file system properties -->

  <property>
    <name>fs.defaultFS</name>
    <!-- cluster variant -->
    <value>hdfs://apollo-phx-nn-ha</value>
    <description>The name of the default file system.  Either the
  literal string "local" or a host:port for NDFS.</description>
    <final>true</final>
  </property>

  <property>
    <description>Topology script</description>
    <name>net.topology.script.file.name</name>
    <value>/apache/hadoop/etc/hadoop/topology</value>
    <final>true</final>
  </property>

  <property>
    <name>fs.trash.interval</name>
    <value>480</value>
    <description>Number of minutes between trash checkpoints.
                 If zero, the trash feature is disabled.
    </description>
  </property>	

  <!-- mobius-proxyagent impersonation configurations -->
<property>
  <name>hadoop.proxyuser.mobius-proxyagent.groups</name>
  <value>hdmi-mm,hdmi-set,hdmi-research,hdmi-technology,hdmi-hadoopeng,hdmi-cs,hdmi-milo,hdmi-appdev,hdmi-siteanalytics,hdmi-prod,hdmi-others,hdmi-sdc,hdmi-finance,hdmi-est,hdmi-cci,hdmi-mptna,hdmi-xcom,hdmi-stu,hdmi-mobile</value>
  <description>Allow user mobius-proxyagent to impersonate any members of the groups </description>
</property>

<property>
    <name>hadoop.proxyuser.mobius-proxyagent.hosts</name>
    <value>10.114.118.13,10.115.201.53</value>
    <description>The mobius-proxyagent can connect from hosts to impersonate a user</description>
</property>

<property>
      <name>hadoop.proxyuser.bridge_adm.groups</name>
      <value>hdmi-mm,hdmi-set,hdmi-research,hdmi-technology,hdmi-hadoopeng,hdmi-cs,hdmi-milo,hdmi-appdev,hdmi-siteanalytics,hdmi-prod,hdmi-others,hdmi-sdc,hdmi-finance,hdmi-est,hdmi-cci,hdmi-mptna,hdmi-xcom,hdmi-stu,hdmi-mobile</value>
      <description>Allow user bridge_adm (Teradata-Hadoop bridge) to impersonate any members of the groups </description>
</property>

<property>
    <name>hadoop.proxyuser.bridge_adm.hosts</name>
    <value>10.103.47.11,10.103.47.12,10.103.47.13,10.103.47.14,10.103.47.15,10.103.47.16,10.103.47.17,10.103.47.18,10.103.47.19,10.103.47.20,10.103.47.21,10.103.47.22,10.103.48.11,10.103.48.12,10.103.48.13,10.103.48.14,10.103.48.15,10.103.48.16,10.103.48.17,10.103.48.18,10.103.48.19,10.103.48.20,10.103.48.21,10.103.48.22,10.103.88.11,10.103.88.12,10.103.88.13,10.103.88.14,10.103.88.15,10.103.88.16,10.103.88.17,10.103.88.18,10.103.88.19,10.103.88.20,10.103.88.21,10.103.88.22,10.103.88.23,10.103.88.24,10.103.88.25,10.103.88.26,10.103.88.27,10.103.88.28,10.103.88.29,10.103.88.30,10.103.88.31,10.103.88.32,10.103.88.33,10.103.88.34,10.103.89.11,10.103.89.12,10.103.89.13,10.103.89.14,10.103.89.15,10.103.89.16,10.103.89.17,10.103.89.18,10.103.89.19,10.103.89.20,10.103.89.21,10.103.89.22,10.103.89.23,10.103.89.24,10.103.89.25,10.103.89.26,10.103.89.27,10.103.89.28,10.103.89.29,10.103.89.30,10.103.89.31,10.103.89.32,10.103.89.33,10.103.89.34,10.115.37.50,10.115.37.51,10.115.37.52,10.115.37.53,10.115.38.50,10.115.38.51,10.115.38.52,10.115.38.53,10.115.208.11,10.115.208.12,10.115.208.13,10.115.208.14,10.115.208.15,10.115.208.16,10.115.208.17,10.115.208.18,10.115.208.19,10.115.208.20,10.115.208.21,10.115.208.22,10.115.208.23,10.115.208.24,10.115.208.25,10.115.208.26,10.103.158.101,10.103.158.102,10.103.158.103,10.103.158.104,10.103.158.105,10.103.158.106,10.103.158.107,10.103.158.108,10.103.158.109,10.103.158.110,10.103.158.111,10.103.158.112,10.103.158.113,10.103.158.114,10.103.158.115,10.103.158.116</value>
    <description>The bridge_adm user (Teradata-Hadoop bridge) can connect from hosts to impersonate a user</description>
</property>

<property>
    <name>hadoop.proxyuser.hadoop.hosts</name>
    <value>*</value>
</property>

<property>
    <name>hadoop.proxyuser.hadoop.groups</name>
    <value>*</value>
</property>

<property>
   <name>hadoop.proxyuser.sg_adm.groups</name>
   <value>hdmi-etl</value>
   <description>Allow user sg_adm (HDMIT-4462) to impersonate any  members of the groups </description>
</property>

<property>
   <name>hadoop.proxyuser.sg_adm.hosts</name>
   <value>*</value>
   <description>The sg_adm user (HDMIT-4462) can connect from hosts to impersonate a user</description>
</property>

  <property>
    <name>fs.inmemory.size.mb</name>
    <value>256</value>
  </property>

  <!-- ipc properties: copied from kryptonite configuration -->
  <property>
    <name>ipc.client.idlethreshold</name>
    <value>8000</value>
    <description>Defines the threshold number of connections after which
               connections will be inspected for idleness.
  </description>
  </property>

  <property>
    <name>ipc.client.connection.maxidletime</name>
    <value>30000</value>
    <description>The maximum time after which a client will bring down the
               connection to the server.
  </description>
  </property>

  <property>
    <name>ipc.client.connect.max.retries</name>
    <value>50</value>
    <description>Defines the maximum number of retries for IPC connections.</description>
  </property>

  <!-- Web Interface Configuration -->
  <property>
    <name>webinterface.private.actions</name>
    <value>false</value>
    <description> If set to true, the web interfaces of JT and NN may contain
                actions, such as kill job, delete file, etc., that should
                not be exposed to public. Enable this option if the interfaces
                are only reachable by those who have the right authorization.
  </description>
  </property>

<property>
  <name>hadoop.proxyuser.hive.groups</name>
  <value>*</value>
  <description>
     Proxy group for Hadoop.
  </description>
</property>

<property>
  <name>hadoop.proxyuser.hive.hosts</name>
  <value>*</value>
  <description>
     Proxy host for Hadoop.
  </description>
</property>

<property>
  <name>hadoop.proxyuser.oozie.groups</name>
  <value>*</value>
  <description>
     Proxy group for Hadoop.
  </description>
</property>

<property>
  <name>hadoop.proxyuser.oozie.hosts</name>
  <value>phxaishdc9en0007-be.phx.ebay.com</value>
  <description>
     Proxy host for Hadoop.
  </description>
</property>

<!-- BEGIN security configuration -->
  <property>
    <name>hadoop.security.authentication</name>
    <value>kerberos</value>
    <!-- A value of "simple" would  disable security. -->
  </property>
  
  <property>
    <name>hadoop.security.authorization</name>
    <value>true</value>
  </property>

  <!-- Setting to ShellBasedUnixGroupsMapping to override the default of 
       JniBasedUnixGroupsMappingWithFallback.  See HWX case 00006991 -->
  <property>
    <name>hadoop.security.group.mapping</name>
    <value>org.apache.hadoop.security.ShellBasedUnixGroupsMapping</value>
  </property>

  <property>
    <name>hadoop.http.filter.initializers</name>
    <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
  </property>

<!-- BEGIN hadoop.http.authentication properties --> 
  <property>
    <name>hadoop.http.authentication.type</name>
    <value>org.apache.hadoop.security.authentication.server.CompositeAuthenticationHandler</value>
  </property>

  <property>
    <name>hadoop.http.authentication.token.validity</name>
    <value>36000</value>
    <!-- in seconds -->
  </property>

  <property>
    <name>hadoop.http.authentication.signature.secret.file</name>
    <value>/etc/hadoop/http_auth_secret</value>
  </property>

  <property>
    <name>hadoop.http.authentication.cookie.domain</name>
    <value>ebay.com</value>
  </property>

  <property>
    <name>hadoop.http.authentication.pingFederate.config.file</name>
    <value>/etc/hadoop/pingfederate-agent-config.txt</value>
  </property>

  <property>
    <name>hadoop.http.authentication.pingFederate.url</name>
    <value>https://sso.corp.ebay.com/sp/startSSO.ping?PartnerIdpId=eBayHadoop</value>
  </property>

  <property>
    <name>hadoop.http.authentication.pingFederate.anonymous.allowed</name>
    <value>true</value>
  </property>

<!-- BEGIN properties enabled per HDP-2.1.3 upgrade -->

  <property>
    <name>hadoop.http.authentication.composite.handlers</name>
    <value>org.apache.hadoop.security.authentication.server.PingFederateAuthenticationHandler,kerberos,anonymous</value>
  </property>

  <property>
    <name>hadoop.http.authentication.composite.default-non-browser-handler-type</name>
    <value>kerberos</value>
  </property>

  <property>
    <name>hadoop.http.authentication.kerberos.keytab</name>
    <value>/etc/hadoop/hadoop.keytab</value>
  </property>

  <property>
    <name>hadoop.http.authentication.kerberos.principal</name>
    <value>*</value>
  </property>

<!-- END properties enabled per HDP-2.1.3 upgrade -->

<!-- END hadoop.http.authentication properties --> 


  <property>
    <name>hadoop.security.auth_to_local</name>
    <value>
        RULE:[1:$1]
        RULE:[2:$1]
        DEFAULT
    </value>
  </property>

  <property>
    <name>kerberos.multiplerealm.supported</name>
    <value>true</value>
  </property>
  
  <property>
    <name>kerberos.multiplerealm.realms</name>
    <value>CORP.EBAY.COM</value>
  </property>

<!--SSL SUPPORT -->

<property>
  <name>hadoop.ssl.keystores.factory.class</name>
    <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
    <description>
          The keystores factory to use for retrieving certificates.
    </description>
</property>

<property>
  <name>hadoop.ssl.require.client.cert</name>
  <value>false</value>
  <description>Whether client certificates are required</description>
</property>

<property>
  <name>hadoop.ssl.hostname.verifier</name>
  <value>ALLOW_ALL</value>
  <description>
    The hostname verifier to provide for HttpsURLConnections.
    Valid values are: DEFAULT, STRICT, STRICT_I6, DEFAULT_AND_LOCALHOST and
    ALLOW_ALL
  </description>
</property>

<property>
  <name>hadoop.ssl.server.conf</name>
  <value>ssl-server.xml</value>
  <description>
    Resource file from which ssl server keystore information will be extracted.
    This file is looked up in the classpath, typically it should be in Hadoop
    conf/ directory.
  </description>
</property>

<property>
  <name>hadoop.ssl.client.conf</name>
  <value>ssl-client.xml</value>
  <description>
    Resource file from which ssl client keystore information will be extracted
    This file is looked up in the classpath, typically it should be in Hadoop
    conf/ directory.
  </description>
</property>

<property>
  <name>hadoop.ssl.enabled</name>
  <value>false</value>
  <description>
    Whether to use SSL for the HTTP endpoints. If set to true, the
    NameNode, DataNode, ResourceManager, NodeManager, HistoryServer and
    MapReduceAppMaster web UIs will be served over HTTPS instead HTTP.
  </description>
</property>

<!-- User Group Resolution -->

<property>
    <name>hadoop.security.groups.cache.secs</name>
    <value>3600</value>
</property>

<!-- END security configuration -->



<!-- BEGIN properties enabled per HDP-2.1.3 upgrade -->

<!-- BEGIN Quality of Service -->

  <property>
    <name>ipc.8020.callqueue.impl</name>
    <value>com.ebay.hadoop.ipc.FairCallQueue</value>
  </property>

  <property>
    <name>ipc.8020.identity-provider.impl</name>
    <value>com.ebay.hadoop.ipc.EbayUserIdentityProvider</value>
  </property>

  <property>
    <name>ipc.8020.faircallqueue.rpc-scheduler</name>
    <value>com.ebay.hadoop.ipc.DecayRpcScheduler</value>
  </property>

  <property>
    <name>ipc.8020.faircallqueue.priority-levels</name>
    <value>10</value>
  </property>

  <property>
    <name>ipc.8020.faircallqueue.decay-scheduler.thresholds</name>
   <!-- <value>1,2,7,10,20,30,40,50,60</value> -->
    <value>1,2,3,5,8,13,20,35,50</value>
  </property>

  <property>
    <name>ipc.8020.faircallqueue.decay-scheduler.period-ms</name>
    <value>1000</value>
  </property>

  <property>
    <name>ipc.8020.faircallqueue.multiplexer.weights</name>
   <!-- <value>10,5,3,2,1,1,1,1,1,1</value> -->
     <value>80,30,25,20,17,12,6,3,2,1</value>
  </property>

<!-- END Quality of Service -->



<!-- BEGIN Selective Encryption --> 
<!-- disabled per HADP-6065 - miguenther - 26 August 2014 
  <property>
    <name>hadoop.rpc.protection</name>
    <value>authentication,privacy</value>
    <final>true</final>
  </property>

  <property>
    <name>hadoop.security.saslproperties.resolver.class</name>
    <value>org.apache.hadoop.security.WhitelistBasedResolver</value>
    <final>true</final>
  </property>

  <property>
    <name>hadoop.security.sasl.variablewhitelist.enable</name>
    <value>true</value>
    <final>true</final>
  </property>
-->
<!-- END Selective Encryption -->


<!-- END properties enabled per HDP-2.1.3 upgrade -->

<property>
  <name>ha.zookeeper.quorum</name>
  <value>apollo-phx-zk-1.vip.ebay.com:2181,apollo-phx-zk-2.vip.ebay.com:2181,apollo-phx-zk-3.vip.ebay.com:2181,apollo-phx-zk-4.vip.ebay.com:2181,apollo-phx-zk-5.vip.ebay.com:2181</value>
</property>

<!-- NEW QOP Proposed configs below - Same as Ares Tiffany Sept 01, 2015 -->
<property>
    <name>hadoop.rpc.protection</name>
    <value>authentication,privacy</value>
</property>

  <property>
      <name>hadoop.security.saslproperties.resolver.class</name>
      <value>org.apache.hadoop.security.WhitelistBasedResolver</value>
  </property>

  <property>
      <name>hadoop.security.sasl.fixedwhitelist.file</name>
      <value>/etc/hadoop/fixedwhitelist</value>
  </property>

  <property>
      <name>hadoop.security.sasl.variablewhitelist.enable</name>
      <value>true</value>
  </property>

  <property>
      <name>hadoop.security.sasl.variablewhitelist.file</name>
      <value>/etc/hadoop/whitelist</value>
  </property>

  <property>
        <name>hadoop.security.sasl.variablewhitelist.cache.secs</name>
        <value>3600</value>
  </property>
<!-- END NEW QOP Proposed configs below - Same as Ares Tiffany Sept 01, 2015 -->

</configuration>
