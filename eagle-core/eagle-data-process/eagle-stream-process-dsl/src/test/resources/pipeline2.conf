{
	config {

	}

	schema {
		metricStreamSchema {
			metric: string
			value: double
			timestamp: long
		}
	}

	dataflow {
		kafkaSource.metric_event_1 {
			codec = "json"
			schema = "metricStreamSchema"
			parallism = 1000
			topic = "metric_event_1"
			zkConnection = "localhost:2181"
			zkConnectionTimeoutMS = 15000
			consumerGroupId = "Consumer"
			fetchSize = 1048586
			transactionZKServers = "localhost"
			transactionZKPort = 2181
			transactionZKRoot = "/consumers"
			transactionStateUpdateMS = 2000
			deserializerClass = "org.apache.eagle.datastream.storm.JsonMessageDeserializer"
		}

		kafkaSource.metric_event_2 {
			schema = "metricStreamSchema"
			parallism = 1000
			topic = "metric_event_2"
			zkConnection = "localhost:2181"
			zkConnectionTimeoutMS = 15000
			consumerGroupId = "Consumer"
			fetchSize = 1048586
			transactionZKServers = "localhost"
			transactionZKPort = 2181
			transactionZKRoot = "/consumers"
			transactionStateUpdateMS = 2000
			deserializerClass = "org.apache.eagle.datastream.storm.JsonMessageDeserializer"
		}

		kafkaSink.metricStore {
			schema = "metricStreamSchema"
			parallism = 1000
			topic = "metric_event_2"
			zkConnection = "localhost:2181"
			zkConnectionTimeoutMS = 15000
			consumerGroupId = "Consumer"
			fetchSize = 1048586
			transactionZKServers = "localhost"
			transactionZKPort = 2181
			transactionZKRoot = "/consumers"
			transactionStateUpdateMS = 2000
			deserializerClass = "org.apache.eagle.datastream.storm.JsonMessageDeserializer"
		}

		alert.alert {
			executor = "alertExecutor"
		}

		aggregator.aggreator {
			executor = "aggreationExecutor"
		}

		metric_event_1|metric_event_2 -> alert {
			grouping = "shuffle"
		}

		metric_event_1|metric_event_2 -> metricStore {
			grouping = "shuffle"
		}
	}
}